{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projeto de Machine Learning com o Brazilian E-Commerce Public Dataset by Olist\n",
    "# Etapa 1: Importação de bibliotecas e carregamento dos dados\n",
    "\n",
    "# Bibliotecas para manipulação, visualização e machine learning\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score, silhouette_score, davies_bouldin_score)\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Extração dos dados do zip (caso ainda não tenha sido extraído)\n",
    "if not os.path.exists('olist_data'):\n",
    "    with zipfile.ZipFile('olist.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('olist_data')\n",
    "\n",
    "# Listar arquivos extraídos\n",
    "files = sorted(os.listdir('olist_data'))\n",
    "csv_files = [f for f in files if f.endswith('.csv')]\n",
    "print('Arquivos CSV disponíveis:', csv_files)\n",
    "\n",
    "# Carregar os principais datasets\n",
    "orders = pd.read_csv('olist_data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('olist_data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('olist_data/olist_products_dataset.csv')\n",
    "payments = pd.read_csv('olist_data/olist_order_payments_dataset.csv')\n",
    "reviews = pd.read_csv('olist_data/olist_order_reviews_dataset.csv')\n",
    "sellers = pd.read_csv('olist_data/olist_sellers_dataset.csv')\n",
    "customers = pd.read_csv('olist_data/olist_customers_dataset.csv')\n",
    "geolocation = pd.read_csv('olist_data/olist_geolocation_dataset.csv')\n",
    "category_translation = pd.read_csv('olist_data/product_category_name_translation.csv')\n",
    "\n",
    "# Visualizar as primeiras linhas do dataset central\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a64d9",
   "metadata": {},
   "source": [
    "# Projeto de Machine Learning: Olist E-Commerce Dataset\n",
    "\n",
    "Este notebook apresenta um projeto completo de ciência de dados utilizando o Brazilian E-Commerce Public Dataset by Olist. O objetivo é realizar uma análise exploratória detalhada, propor e resolver dois problemas de negócio críticos para a operação da Olist, implementar diferentes algoritmos de machine learning, e extrair insights acionáveis para melhorar a operação do marketplace.\n",
    "\n",
    "## Problemas\n",
    "\n",
    "1. Problema Supervisionado: Previsão do Tempo de Entrega\n",
    "Objetivo: Criar um modelo que prevê o tempo exato de entrega (em dias) para cada pedido específico, considerando:\n",
    "- Distância real entre vendedor e cliente (dados de geolocalização)\n",
    "- Histórico de entregas do vendedor\n",
    "- Tamanho/peso do produto\n",
    "- Condições sazonais e região\n",
    "- Performance do transportador\n",
    "Justificativa: Precisão no prazo de entrega é crucial para satisfação do cliente e eficiência operacional.\n",
    "Valor de Negócio: \n",
    "- Redução de reclamações por atrasos\n",
    "- Melhor gestão de expectativas do cliente\n",
    "- Otimização da logística\n",
    "- Possibilidade de oferecer frete expresso com maior precisão\n",
    "- Redução de custos com compensações por atrasos\n",
    "\n",
    "2. Problema Não Supervisionado: Detecção de Vendedores Problemáticos\n",
    "Objetivo: Identificar automaticamente vendedores suspeitos usando técnicas de detecção de anomalias, analisando:\n",
    "- Padrões anormais de preços\n",
    "- Taxa de cancelamento e devolução\n",
    "- Velocidade de crescimento suspeita nas vendas\n",
    "- Concentração geográfica incomum de avaliações\n",
    "- Padrões de texto suspeitos em avaliações\n",
    "- Histórico de reclamações\n",
    "Justificativa: Proteção da reputação do marketplace e garantia de qualidade dos vendedores.\n",
    "Valor de Negócio:\n",
    "- Redução de fraudes e prejuízos\n",
    "- Melhoria na qualidade geral dos vendedores\n",
    "- Aumento da confiança dos compradores\n",
    "- Redução de custos com suporte\n",
    "\n",
    "Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27d2fe",
   "metadata": {},
   "source": [
    "# 2. Análise Exploratória dos Dados (EDA)\n",
    "\n",
    "Nesta seção, vamos explorar os principais datasets do projeto, analisando estatísticas descritivas, valores ausentes, distribuições e relações relevantes entre as tabelas. O objetivo é compreender o contexto dos dados e identificar potenciais variáveis para os problemas de negócio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41680670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar informações gerais dos principais datasets\n",
    "def resumo_dataset(df, nome):\n",
    "    print(f'\\nResumo do dataset: {nome}')\n",
    "    display(df.info())\n",
    "    display(df.describe(include='all'))\n",
    "    print(f'Valores ausentes em {nome}:')\n",
    "    display(df.isnull().sum())\n",
    "\n",
    "resumo_dataset(orders, 'orders')\n",
    "resumo_dataset(order_items, 'order_items')\n",
    "resumo_dataset(products, 'products')\n",
    "resumo_dataset(payments, 'payments')\n",
    "resumo_dataset(reviews, 'reviews')\n",
    "resumo_dataset(sellers, 'sellers')\n",
    "resumo_dataset(customers, 'customers')\n",
    "resumo_dataset(geolocation, 'geolocation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25412c4",
   "metadata": {},
   "source": [
    "# 3. Limpeza e Tratamento dos Dados\n",
    "\n",
    "Nesta etapa, vamos tratar valores ausentes, inconsistências e preparar os dados para a modelagem. Isso inclui:\n",
    "- Remoção ou imputação de valores nulos\n",
    "- Conversão de tipos de dados\n",
    "- Ajuste de colunas de datas\n",
    "- Verificação de duplicidades\n",
    "- Padronização de categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e17cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento de valores ausentes e tipos de dados\n",
    "# Exemplo para o dataset de pedidos (orders)\n",
    "\n",
    "# Converter colunas de datas\n",
    "orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'])\n",
    "orders['order_approved_at'] = pd.to_datetime(orders['order_approved_at'])\n",
    "orders['order_delivered_carrier_date'] = pd.to_datetime(orders['order_delivered_carrier_date'])\n",
    "orders['order_delivered_customer_date'] = pd.to_datetime(orders['order_delivered_customer_date'])\n",
    "orders['order_estimated_delivery_date'] = pd.to_datetime(orders['order_estimated_delivery_date'])\n",
    "\n",
    "# Verificar valores ausentes\n",
    "print('Valores ausentes em orders:')\n",
    "display(orders.isnull().sum())\n",
    "\n",
    "# Exemplo de tratamento: remover linhas sem data de entrega real (para problemas supervisionados)\n",
    "orders_clean = orders.dropna(subset=['order_delivered_customer_date'])\n",
    "\n",
    "# Repetir processo para outros datasets conforme necessário\n",
    "# Exemplo: preencher valores nulos em reviews\n",
    "reviews['review_comment_message'] = reviews['review_comment_message'].fillna('Sem comentário')\n",
    "reviews['review_comment_title'] = reviews['review_comment_title'].fillna('Sem título')\n",
    "\n",
    "# Conferir duplicidades\n",
    "print('Pedidos duplicados:', orders_clean.duplicated(subset=['order_id']).sum())\n",
    "print('Clientes duplicados:', customers.duplicated(subset=['customer_id']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da quantidade de pedidos por estado do cliente\n",
    "clientes_estados = customers.merge(orders, on='customer_id')\n",
    "clientes_estados = clientes_estados['customer_state'].value_counts().reset_index()\n",
    "clientes_estados.columns = ['Estado', 'Quantidade de Pedidos']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=clientes_estados, x='Estado', y='Quantidade de Pedidos', palette='viridis')\n",
    "plt.title('Quantidade de Pedidos por Estado do Cliente')\n",
    "plt.xlabel('Estado')\n",
    "plt.ylabel('Pedidos')\n",
    "plt.show()# Visualização de distribuições e relações iniciais dos dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Distribuição do status dos pedidos\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=orders, x='order_status', order=orders['order_status'].value_counts().index)\n",
    "plt.title('Distribuição dos Status dos Pedidos')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.show()\n",
    "\n",
    "# Distribuição das avaliações dos clientes\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=reviews, x='review_score', order=sorted(reviews['review_score'].unique()))\n",
    "plt.title('Distribuição das Avaliações dos Clientes')\n",
    "plt.xlabel('Nota de Avaliação')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.show()\n",
    "\n",
    "# Distribuição dos tipos de pagamento\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=payments, x='payment_type', order=payments['payment_type'].value_counts().index)\n",
    "plt.title('Distribuição dos Tipos de Pagamento')\n",
    "plt.xlabel('Tipo de Pagamento')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d76fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ced4d7c",
   "metadata": {},
   "source": [
    "# 4. Definição dos Problemas de Negócio\n",
    "\n",
    "Nesta seção, serão definidos dois problemas de negócio relevantes para a Olist:\n",
    "\n",
    "- **Problema supervisionado:** Previsão da nota de avaliação do cliente (regressão). O objetivo é prever a nota que um cliente dará ao pedido, com base em informações do pedido, produto, entrega e pagamento. Isso permite à Olist identificar fatores que impactam a satisfação do cliente e atuar preventivamente para melhorar a experiência.\n",
    "\n",
    "- **Problema não supervisionado:** Segmentação de clientes por comportamento de compra (clustering). O objetivo é agrupar clientes com perfis de compra semelhantes, permitindo ações de marketing direcionadas, personalização de ofertas e melhor compreensão da base de clientes.\n",
    "\n",
    "A seguir, detalharemos as variáveis utilizadas, justificativas e valor de negócio de cada abordagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429c9ed",
   "metadata": {},
   "source": [
    "## 4.1 Problema Supervisionado: Previsão da Nota de Avaliação do Cliente\n",
    "\n",
    "**Justificativa:**\n",
    "- A nota de avaliação é um indicador direto da satisfação do cliente.\n",
    "- Antecipar avaliações baixas permite ações proativas para reduzir churn e melhorar a reputação.\n",
    "\n",
    "**Variáveis sugeridas:**\n",
    "- Prazo de entrega (real x estimado)\n",
    "- Valor do pedido\n",
    "- Tipo de pagamento\n",
    "- Quantidade de itens\n",
    "- Categoria do produto\n",
    "- Região do cliente\n",
    "- Status do pedido\n",
    "\n",
    "**Valor de negócio:**\n",
    "- Redução de avaliações negativas\n",
    "- Melhoria da experiência do cliente\n",
    "- Aumento da fidelização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625f2a9",
   "metadata": {},
   "source": [
    "## 4.2 Problema Não Supervisionado: Segmentação de Clientes por Comportamento de Compra\n",
    "\n",
    "**Justificativa:**\n",
    "- A Olist pode personalizar campanhas e ofertas para diferentes perfis de clientes.\n",
    "- Permite identificar grupos de alto valor, clientes recorrentes, caçadores de promoções, etc.\n",
    "\n",
    "**Variáveis sugeridas:**\n",
    "- Frequência de compras\n",
    "- Ticket médio\n",
    "- Diversidade de categorias compradas\n",
    "- Região\n",
    "- Forma de pagamento\n",
    "\n",
    "**Valor de negócio:**\n",
    "- Aumento da efetividade de marketing\n",
    "- Personalização de ofertas\n",
    "- Melhoria do relacionamento com o cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3dca4c",
   "metadata": {},
   "source": [
    "Na próxima etapa, os dados serão preparados para a modelagem, incluindo seleção de variáveis, criação de features e junção das tabelas necessárias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282cff0",
   "metadata": {},
   "source": [
    "# 5. Preparação dos Dados para Modelagem\n",
    "\n",
    "Nesta etapa, vamos preparar os dados para os dois problemas definidos:\n",
    "- Seleção e criação de variáveis (features)\n",
    "- Junção das tabelas necessárias\n",
    "- Engenharia de atributos\n",
    "- Padronização e encoding\n",
    "\n",
    "Primeiro, será feita a preparação para o problema supervisionado (previsão da nota de avaliação do cliente), seguida da preparação para o problema não supervisionado (segmentação de clientes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106dae78",
   "metadata": {},
   "source": [
    "## 5.1 Preparação para o Problema Supervisionado\n",
    "\n",
    "Vamos criar um dataset unificado contendo, para cada pedido avaliado:\n",
    "- Informações do pedido (datas, status, região)\n",
    "- Informações do pagamento\n",
    "- Informações do produto principal\n",
    "- Prazo de entrega real x estimado\n",
    "- Nota de avaliação (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2949f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação para o problema de previsão de tempo de entrega\n",
    "# 1. Juntar as tabelas necessárias\n",
    "df_sup = orders_clean.merge(order_items, on='order_id')\n",
    "df_sup = df_sup.merge(payments, on='order_id', how='left')  # Adiciona informações de pagamento\n",
    "df_sup = df_sup.merge(products, on='product_id', how='left')\n",
    "df_sup = df_sup.merge(category_translation, left_on='product_category_name', right_on='product_category_name', how='left')  # Adiciona coluna em inglês\n",
    "df_sup = df_sup.merge(sellers[['seller_id', 'seller_city', 'seller_state']], on='seller_id', how='left')\n",
    "df_sup = df_sup.merge(customers[['customer_id', 'customer_city', 'customer_state']], on='customer_id', how='left')\n",
    "df_sup = df_sup.merge(reviews[['order_id', 'review_score']], on='order_id', how='left')  # apenas um merge\n",
    "\n",
    "# 2. Calcular a média de entrega por vendedor\n",
    "seller_orders = orders_clean.merge(order_items[['order_id', 'seller_id']], on='order_id')\n",
    "seller_performance = seller_orders.groupby('seller_id').apply(\n",
    "    lambda g: (g['order_delivered_customer_date'] - g['order_purchase_timestamp']).dt.days.mean()\n",
    ").to_frame('seller_avg_delivery_time')\n",
    "df_sup = df_sup.merge(seller_performance, on='seller_id', how='left')\n",
    "\n",
    "# 3. Calcular a média de entrega por região (estado)\n",
    "orders_with_state = orders_clean.merge(customers[['customer_id', 'customer_state']], on='customer_id')\n",
    "state_performance = orders_with_state.groupby('customer_state').apply(\n",
    "    lambda g: (g['order_delivered_customer_date'] - g['order_purchase_timestamp']).dt.days.mean()\n",
    ").to_frame('state_avg_delivery_time')\n",
    "df_sup = df_sup.merge(state_performance, on='customer_state', how='left')\n",
    "\n",
    "# 4. Feature engineering para informações do produto\n",
    "df_sup['product_volume_cm3'] = df_sup['product_length_cm'] * df_sup['product_height_cm'] * df_sup['product_width_cm']\n",
    "df_sup['product_density'] = df_sup['product_weight_g'] / df_sup['product_volume_cm3']\n",
    "\n",
    "# 5. Feature engineering para distância (usando cidade/estado como proxy)\n",
    "df_sup['same_state'] = (df_sup['seller_state'] == df_sup['customer_state']).astype(int)\n",
    "df_sup['same_city'] = (df_sup['seller_city'] == df_sup['customer_city']).astype(int)\n",
    "\n",
    "# Feature engineering: prazo de entrega real x estimado (em dias)\n",
    "df_sup['delivery_time'] = (df_sup['order_delivered_customer_date'] - df_sup['order_purchase_timestamp']).dt.days\n",
    "df_sup['delivery_delay'] = (df_sup['order_delivered_customer_date'] - df_sup['order_estimated_delivery_date']).dt.days\n",
    "\n",
    "# Agora sim, tratar review_score (garantindo que existe)\n",
    "if 'review_score' in df_sup.columns:\n",
    "    df_sup['review_score'] = df_sup['review_score'].fillna(df_sup['review_score'].mean())\n",
    "\n",
    "# Remoção de valores nulos em todos os campos importantes\n",
    "df_sup = df_sup.dropna(subset=['review_score', 'delivery_time', 'delivery_delay'])\n",
    "\n",
    "# Preencher valores nulos nas categorias com valores de substituição adequados\n",
    "if 'product_category_name_english' in df_sup.columns:\n",
    "    df_sup['product_category_name_english'] = df_sup['product_category_name_english'].fillna('unknown')\n",
    "if 'customer_state' in df_sup.columns:\n",
    "    df_sup['customer_state'] = df_sup['customer_state'].fillna(df_sup['customer_state'].mode()[0])\n",
    "if 'payment_type' in df_sup.columns:\n",
    "    df_sup['payment_type'] = df_sup['payment_type'].fillna(df_sup['payment_type'].mode()[0])\n",
    "if 'order_status' in df_sup.columns:\n",
    "    df_sup['order_status'] = df_sup['order_status'].fillna('delivered')  # Assumindo que todos os pedidos com revisão foram entregues\n",
    "\n",
    "# Preencher valores nulos nas variáveis numéricas com a mediana\n",
    "if 'payment_value' in df_sup.columns:\n",
    "    df_sup['payment_value'] = df_sup['payment_value'].fillna(df_sup['payment_value'].median())\n",
    "\n",
    "# Selecionar features finais para modelagem\n",
    "features_sup = [\n",
    "    'payment_value', 'payment_type', 'customer_state', 'product_category_name_english',\n",
    "    'delivery_time', 'delivery_delay', 'order_status', 'review_score',\n",
    "    'product_weight_g',      # Adicionada para regressão\n",
    "    'product_length_cm',     # Adicionada para regressão\n",
    "    'product_height_cm',     # Adicionada para regressão\n",
    "    'product_width_cm',      # Adicionada para regressão\n",
    "    'freight_value',         # Adicionada para regressão\n",
    "    'product_volume_cm3',    # Adicionada para regressão\n",
    "    'product_density',       # Adicionada para regressão\n",
    "    'seller_avg_delivery_time', # Adicionada para regressão\n",
    "    'state_avg_delivery_time',  # Adicionada para regressão\n",
    "    'same_state',               # Adicionada para regressão\n",
    "    'same_city'                 # Adicionada para regressão\n",
    "]\n",
    "# A linha abaixo foi comentada pois a filtragem específica de colunas\n",
    "# deve acontecer dentro dos pipelines de cada modelo, se necessário,\n",
    "# ou ao preparar X_class e X_reg separadamente.\n",
    "# df_sup = df_sup[features_sup]\n",
    "\n",
    "# Verificar se ainda existem valores nulos\n",
    "print(\"Valores nulos por coluna após tratamento:\")\n",
    "print(df_sup.isnull().sum())\n",
    "\n",
    "df_sup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af798f",
   "metadata": {},
   "source": [
    "## 5.2 Preparação para o Problema Não Supervisionado\n",
    "\n",
    "Vamos criar um dataset agregando informações por cliente:\n",
    "- Frequência de compras\n",
    "- Ticket médio\n",
    "- Diversidade de categorias\n",
    "- Região\n",
    "- Forma de pagamento mais comum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd02719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação para detecção de vendedores problemáticos\n",
    "\n",
    "# Verificar se as variáveis necessárias existem e recriá-las se necessário\n",
    "try:\n",
    "    # Testar se orders_clean existe\n",
    "    type(orders_clean)\n",
    "except NameError:\n",
    "    # Se não existe, criá-la a partir de orders\n",
    "    print(\"Criando variável orders_clean...\")\n",
    "    orders_clean = orders.dropna(subset=['order_delivered_customer_date'])\n",
    "\n",
    "# 1. Métricas básicas por vendedor\n",
    "df_unsup = order_items.merge(orders_clean, on='order_id')\n",
    "df_unsup = df_unsup.merge(reviews[['order_id', 'review_score', 'review_comment_message']], on='order_id', how='left')\n",
    "\n",
    "# Agregar métricas por vendedor (nivelando a hierarquia do resultado com flatten_names=True)\n",
    "vendedor_metricas = df_unsup.groupby('seller_id').agg({\n",
    "    'order_id': 'count',  # Volume de vendas\n",
    "    'price': ['mean', 'std'],  # Média e desvio padrão dos preços\n",
    "    'review_score': ['mean', 'count'],  # Média e quantidade de avaliações\n",
    "    'order_status': lambda x: (x == 'canceled').mean()  # Taxa de cancelamento\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten das colunas multi-índice\n",
    "vendedor_metricas.columns = ['seller_id', 'total_vendas', 'preco_medio', 'preco_std', \n",
    "                            'avaliacao_media', 'num_avaliacoes', 'taxa_cancelamento']\n",
    "\n",
    "# 2. Calcular crescimento suspeito (variação percentual mês a mês)\n",
    "vendas_mensais = df_unsup.groupby(['seller_id', pd.Grouper(key='order_purchase_timestamp', freq='M')])['order_id'].count().reset_index()\n",
    "vendas_mensais['crescimento'] = vendas_mensais.groupby('seller_id')['order_id'].pct_change()\n",
    "\n",
    "# Agregar crescimento por vendedor\n",
    "crescimento_suspeito = vendas_mensais.groupby('seller_id')['crescimento'].agg(['mean', 'max']).reset_index()\n",
    "crescimento_suspeito.columns = ['seller_id', 'crescimento_medio', 'crescimento_maximo']\n",
    "\n",
    "# 3. Análise geográfica das avaliações\n",
    "geo_concentracao = df_unsup.merge(customers[['customer_id', 'customer_state']], on='customer_id').\\\n",
    "    groupby(['seller_id', 'customer_state'])['review_score'].agg(['count', 'mean']).reset_index()\n",
    "\n",
    "# Calcular índice de concentração geográfica (% de vendas no estado principal)\n",
    "geo_concentracao_idx = geo_concentracao.groupby('seller_id').apply(\n",
    "    lambda x: x['count'].max() / x['count'].sum()\n",
    ").reset_index()\n",
    "geo_concentracao_idx.columns = ['seller_id', 'concentracao_geografica']\n",
    "\n",
    "# 4. Juntar todas as métricas\n",
    "df_unsup = vendedor_metricas.merge(crescimento_suspeito, on='seller_id', how='left')\n",
    "df_unsup = df_unsup.merge(geo_concentracao_idx, on='seller_id', how='left')\n",
    "\n",
    "# 5. Criar features para detecção de anomalias\n",
    "df_unsup['preco_zscore'] = (df_unsup['preco_medio'] - df_unsup['preco_medio'].mean()) / df_unsup['preco_medio'].std()\n",
    "df_unsup['avaliacao_zscore'] = (df_unsup['avaliacao_media'] - df_unsup['avaliacao_media'].mean()) / df_unsup['avaliacao_media'].std()\n",
    "df_unsup['crescimento_zscore'] = (df_unsup['crescimento_maximo'] - df_unsup['crescimento_maximo'].mean()) / df_unsup['crescimento_maximo'].std()\n",
    "\n",
    "# Criando variáveis para análise de clientes\n",
    "# Frequência de compras por cliente\n",
    "compras_cliente = orders_clean.groupby('customer_id')['order_id'].count().rename('freq_compras')\n",
    "\n",
    "# Ticket médio por cliente\n",
    "df_pagamento_tipo_temp = orders_clean.merge(payments, on='order_id')\n",
    "pagamentos_cliente = df_pagamento_tipo_temp.groupby('customer_id')['payment_value'].mean().rename('ticket_medio')\n",
    "\n",
    "# Diversidade de categorias\n",
    "itens_cliente = orders_clean.merge(order_items, on='order_id')\n",
    "itens_cliente = itens_cliente.merge(products[['product_id', 'product_category_name']], on='product_id', how='left')\n",
    "diversidade = itens_cliente.groupby('customer_id')['product_category_name'].nunique().rename('num_categorias')\n",
    "\n",
    "# Forma de pagamento mais comum\n",
    "pagamento_tipo = df_pagamento_tipo_temp.groupby('customer_id')['payment_type'].agg(lambda x: x.mode()[0]).rename('payment_type')\n",
    "\n",
    "# Região\n",
    "regiao = customers.set_index('customer_id')['customer_state']\n",
    "\n",
    "# Unir tudo\n",
    "df_unsup_clientes = pd.DataFrame(compras_cliente).join([pagamentos_cliente, diversidade, pagamento_tipo, regiao])\n",
    "df_unsup_clientes = df_unsup_clientes.dropna()\n",
    "\n",
    "df_unsup_clientes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4aad9",
   "metadata": {},
   "source": [
    "Os dados estão prontos para a modelagem! A seguir, serão aplicados os algoritmos de machine learning para cada abordagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598d769",
   "metadata": {},
   "source": [
    "# 6. Modelagem Supervisionada: Previsão da Nota de Avaliação\n",
    "\n",
    "Nesta etapa, serão aplicados três algoritmos de regressão para prever a nota de avaliação do cliente:\n",
    "- Regressão Linear (baseline)\n",
    "- Random Forest Regressor\n",
    "- SVR (Support Vector Regressor)\n",
    "\n",
    "Serão utilizadas validação cruzada, regularização e as métricas R², RMSE e MAE. Também serão apresentadas curvas de aprendizado e análise de overfitting/underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento: encoding e padronização\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = df_sup.drop('review_score', axis=1)\n",
    "y = df_sup['delivery_time']  # Corrigido: usar a coluna já existente\n",
    "\n",
    "# Separar variáveis categóricas e numéricas\n",
    "cat_cols = ['seller_state', 'customer_state', 'product_category_name', 'same_state', 'same_city']\n",
    "num_cols = ['product_weight_g', 'product_volume_cm3', 'product_density', \n",
    "            'seller_avg_delivery_time', 'state_avg_delivery_time']\n",
    "\n",
    "# Pipeline de pré-processamento com imputação para garantir que não haja valores nulos\n",
    "# Os imputadores são adicionados para maior segurança, mesmo que já tenhamos tratado os nulos\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), num_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# Verificar valores nulos antes da divisão\n",
    "print(\"Valores nulos em X antes da divisão:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Primeiro fazer um sample de 30% dos dados\n",
    "X_sample = X.sample(frac=0.3, random_state=42)\n",
    "y_sample = y[X_sample.index]\n",
    "\n",
    "# Divisão treino-teste no sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verificar valores nulos após a divisão\n",
    "print(\"Valores nulos em X_train após a divisão:\")\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "print(\"\\nTamanho do dataset original:\", len(X))\n",
    "print(\"Tamanho do sample (30%):\", len(X_sample))\n",
    "print(\"Tamanho do conjunto de treino:\", len(X_train))\n",
    "print(\"Tamanho do conjunto de teste:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cee320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para avaliação dos modelos de regressão\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def avaliar_regressao(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print('Treino:')\n",
    "    print('R²:', r2_score(y_train, y_pred_train))\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    print('MAE:', mean_absolute_error(y_train, y_pred_train))\n",
    "    print('\\nTeste:')\n",
    "    print('R²:', r2_score(y_test, y_pred_test))\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    print('MAE:', mean_absolute_error(y_test, y_pred_test))\n",
    "    return y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Regressão Linear (baseline)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Regressão Linear:')\n",
    "y_pred_train_lr, y_pred_test_lr = avaliar_regressao(pipe_lr, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Comentário explicativo sobre a resolução do problema de valores NaN\n",
    "print(\"\\nNota sobre o tratamento de valores ausentes:\")\n",
    "print(\"Os valores NaN foram tratados em três níveis para garantir robustez:\")\n",
    "print(\"1. No dataframe: preenchimento específico para cada coluna\")\n",
    "print(\"2. No pré-processamento: imputação por mediana (numéricas) e valor mais frequente (categóricas)\")\n",
    "print(\"3. No OneHotEncoder: parâmetro handle_unknown='ignore' para lidar com categorias ausentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "print('Random Forest:')\n",
    "y_pred_train_rf, y_pred_test_rf = avaliar_regressao(pipe_rf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83742a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SVR (Support Vector Regressor)\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipe_svr = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', SVR())\n",
    "])\n",
    "pipe_svr.fit(X_train, y_train)\n",
    "print('SVR:')\n",
    "y_pred_train_svr, y_pred_test_svr = avaliar_regressao(pipe_svr, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ff4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas de aprendizado para Random Forest\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Verificar as dimensões dos dados após o pré-processamento\n",
    "# Isso é útil para confirmar que não perdemos observações devido a valores NaN\n",
    "X_processed = preprocessor.transform(X)\n",
    "print(f\"\\nVerificação final de dimensões:\")\n",
    "print(f\"X original: {X.shape}\")\n",
    "print(f\"X após pré-processamento: {X_processed.shape}\")\n",
    "print(f\"Isso confirma que o tratamento de valores ausentes está funcionando corretamente.\")\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    pipe_rf, X, y, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), random_state=42\n",
    ")\n",
    "\n",
    "train_scores_mean = -train_scores.mean(axis=1)\n",
    "test_scores_mean = -test_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_sizes, train_scores_mean, label='Treino')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Validação')\n",
    "plt.xlabel('Tamanho do Treinamento')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Curva de Aprendizado - Random Forest')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e440b",
   "metadata": {},
   "source": [
    "Os resultados dos modelos supervisionados serão comparados e discutidos na próxima etapa. Em seguida, será realizada a modelagem não supervisionada (clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523a4a0",
   "metadata": {},
   "source": [
    "# 7. Modelagem Não Supervisionada: Clustering de Clientes\n",
    "\n",
    "Nesta etapa, serão aplicadas três técnicas de clustering para segmentação de clientes:\n",
    "- KMeans\n",
    "- DBSCAN\n",
    "- Agglomerative Clustering\n",
    "\n",
    "Também será utilizada a técnica de redução de dimensionalidade (PCA) para visualização dos clusters. As métricas de avaliação incluem Silhouette Score, Davies-Bouldin e Inércia (quando aplicável)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984af4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento: encoding e padronização para clustering\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fazer sample de 30% dos dados para clustering\n",
    "X_unsup = df_unsup.sample(frac=0.3, random_state=42).copy()\n",
    "print(f\"Tamanho do dataset original: {len(df_unsup)}\")\n",
    "print(f\"Tamanho do sample (30%): {len(X_unsup)}\")\n",
    "\n",
    "# Features para detecção de anomalias em vendedores\n",
    "anomaly_features = [\n",
    "    'total_vendas', 'preco_medio', 'preco_std', 'avaliacao_media', \n",
    "    'taxa_cancelamento', 'crescimento_medio', 'crescimento_maximo',\n",
    "    'concentracao_geografica', 'preco_zscore', 'avaliacao_zscore', 'crescimento_zscore'\n",
    "]\n",
    "\n",
    "# Preparar features para detecção de anomalias\n",
    "anomaly_features = [\n",
    "    'total_vendas', 'preco_medio', 'preco_std', 'avaliacao_media', \n",
    "    'taxa_cancelamento', 'crescimento_medio', 'crescimento_maximo',\n",
    "    'concentracao_geografica', 'preco_zscore', 'avaliacao_zscore', 'crescimento_zscore'\n",
    "]\n",
    "\n",
    "# Preprocessamento para detecção de anomalias\n",
    "preprocessor_unsup = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Verificar valores nulos antes do processamento\n",
    "print(\"Valores nulos em X_unsup antes do processamento:\")\n",
    "print(X_unsup[anomaly_features].isnull().sum())\n",
    "\n",
    "# Preparar os dados para clustering\n",
    "X_unsup_proc = preprocessor_unsup.fit_transform(X_unsup[anomaly_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4112ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redução de dimensionalidade para visualização\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_unsup_proc)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], alpha=0.3)\n",
    "plt.title('Clientes no Espaço PCA (sem cluster)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "labels_kmeans = kmeans.fit_predict(X_unsup_proc)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=labels_kmeans, cmap='tab10', alpha=0.5)\n",
    "plt.title('Clusters de Clientes - KMeans')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "# Converter matriz esparsa para array denso se necessário\n",
    "if hasattr(X_unsup_proc, \"toarray\"):\n",
    "    X_unsup_proc_dense = X_unsup_proc.toarray()\n",
    "else:\n",
    "    X_unsup_proc_dense = X_unsup_proc\n",
    "\n",
    "print('Silhouette Score:', silhouette_score(X_unsup_proc, labels_kmeans))\n",
    "print('Davies-Bouldin:', davies_bouldin_score(X_unsup_proc_dense, labels_kmeans))\n",
    "print('Inércia:', kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcab68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=10)\n",
    "labels_dbscan = dbscan.fit_predict(X_unsup_proc)\n",
    "\n",
    "# Create figure and axes for the plot\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "# Generate the scatter plot\n",
    "scatter_plot = ax.scatter(X_pca[:,0], X_pca[:,1], c=labels_dbscan, cmap='tab10', alpha=0.5)\n",
    "\n",
    "# Set title and axis labels\n",
    "ax.set_title('Clusters de Clientes - DBSCAN')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "# Create legend\n",
    "# Get unique sorted cluster labels\n",
    "unique_labels_sorted = sorted(np.unique(labels_dbscan))\n",
    "\n",
    "# Get the actual colors used by scatter plot for each unique label\n",
    "# PathCollection (scatter_plot) is a ScalarMappable, so to_rgba works.\n",
    "colors_for_labels = scatter_plot.to_rgba(unique_labels_sorted)\n",
    "\n",
    "legend_handles = []\n",
    "for i, label_val in enumerate(unique_labels_sorted):\n",
    "    legend_text = f'Cluster {label_val}' if label_val != -1 else 'Ruído' # \"Ruído\" for Noise\n",
    "    legend_handles.append(plt.Line2D([0], [0], marker='o', color='w', # Invisible line\n",
    "                                     markerfacecolor=colors_for_labels[i],\n",
    "                                     markersize=8,\n",
    "                                     label=legend_text))\n",
    "\n",
    "ax.legend(handles=legend_handles, title=\"Legenda\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Filtrar ruído para métricas\n",
    "mask = labels_dbscan != -1\n",
    "if mask.sum() > 1: # Ensure there are at least 2 points in non-noise clusters\n",
    "    # Convert matriz esparsa para array denso se necessário para Davies-Bouldin\n",
    "    if hasattr(X_unsup_proc, \"toarray\"):\n",
    "        X_unsup_proc_dense = X_unsup_proc.toarray()\n",
    "    else:\n",
    "        X_unsup_proc_dense = X_unsup_proc\n",
    "    \n",
    "    # Check if there are enough unique cluster labels (excluding noise) for metrics\n",
    "    unique_cluster_labels = np.unique(labels_dbscan[mask])\n",
    "    if len(unique_cluster_labels) > 1:\n",
    "        print('Silhouette Score:', silhouette_score(X_unsup_proc[mask], labels_dbscan[mask]))\n",
    "        print('Davies-Bouldin:', davies_bouldin_score(X_unsup_proc_dense[mask], labels_dbscan[mask]))\n",
    "    else:\n",
    "        print('Não há clusters suficientes (excluindo ruído) para calcular Silhouette Score ou Davies-Bouldin Score.')\n",
    "else:\n",
    "    print('Poucos clusters ou apenas ruído encontrado pelo DBSCAN. Métricas de cluster não calculadas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Agglomerative Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=4)\n",
    "labels_agg = agg.fit_predict(X_unsup_proc)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=labels_agg, cmap='tab10', alpha=0.5)\n",
    "plt.title('Clusters de Clientes - Agglomerative')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "# Converter matriz esparsa para array denso se necessário\n",
    "if hasattr(X_unsup_proc, \"toarray\"):\n",
    "    X_unsup_proc_dense = X_unsup_proc.toarray()\n",
    "else:\n",
    "    X_unsup_proc_dense = X_unsup_proc\n",
    "\n",
    "print('Silhouette Score:', silhouette_score(X_unsup_proc, labels_agg))\n",
    "print('Davies-Bouldin:', davies_bouldin_score(X_unsup_proc_dense, labels_agg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fd1f8",
   "metadata": {},
   "source": [
    "Os resultados dos clusters serão analisados e comparados na próxima etapa, junto com a discussão dos insights de negócio extraídos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed54c43",
   "metadata": {},
   "source": [
    "# 8. Avaliação Comparativa e Insights de Negócio\n",
    "\n",
    "Nesta etapa, comparamos o desempenho dos modelos supervisionados e dos clusters, discutindo os principais resultados, limitações e implicações para o negócio Olist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33aa004",
   "metadata": {},
   "source": [
    "## 8.1 Avaliação dos Modelos Supervisionados\n",
    "\n",
    "- **Regressão Linear:** Serve como baseline. Resultados esperados: desempenho modesto, sensível a outliers e relações não-lineares.\n",
    "- **Random Forest:** Geralmente apresenta melhor desempenho, lida bem com não-linearidades e variáveis categóricas. Pode apresentar overfitting se não regularizado.\n",
    "- **SVR:** Útil para capturar relações complexas, mas pode ser sensível à escala dos dados e ao tuning de hiperparâmetros.\n",
    "\n",
    "**Comparação das métricas:**\n",
    "- R², RMSE e MAE em treino e teste.\n",
    "- Curva de aprendizado: análise de overfitting/underfitting.\n",
    "\n",
    "**Principais insights:**\n",
    "- Quais variáveis mais impactam a nota do cliente?\n",
    "- O modelo consegue antecipar avaliações baixas com boa precisão?\n",
    "- Possíveis melhorias: tuning, mais features, outros algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4850622",
   "metadata": {},
   "source": [
    "## 8.2 Avaliação dos Modelos Não Supervisionados\n",
    "\n",
    "- **KMeans:** Permite identificar grupos bem definidos, útil para segmentação de marketing.\n",
    "- **DBSCAN:** Detecta grupos de clientes \"fora da curva\" (anomalias), mas pode formar poucos clusters dependendo dos parâmetros.\n",
    "- **Agglomerative:** Útil para hierarquias e dendrogramas, pode revelar subgrupos interessantes.\n",
    "\n",
    "**Comparação das métricas:**\n",
    "- Silhouette Score, Davies-Bouldin, Inércia (KMeans).\n",
    "- Visualização dos clusters no espaço PCA.\n",
    "\n",
    "**Principais insights:**\n",
    "- Existem grupos de clientes recorrentes, de alto ticket ou caçadores de promoções?\n",
    "- Como as regiões e formas de pagamento se distribuem entre os clusters?\n",
    "- Possíveis ações: campanhas segmentadas, ofertas personalizadas, retenção de clientes valiosos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a18686",
   "metadata": {},
   "source": [
    "# 9. Conclusões Finais\n",
    "\n",
    "- O projeto demonstrou a aplicação de técnicas de machine learning supervisionado e não supervisionado no contexto do e-commerce brasileiro.\n",
    "- Modelos supervisionados permitem antecipar avaliações negativas e atuar preventivamente na experiência do cliente.\n",
    "- Modelos de clustering revelam perfis distintos de clientes, apoiando estratégias de marketing e retenção.\n",
    "- O uso de múltiplos algoritmos e métricas garante robustez e confiabilidade nas análises.\n",
    "- Recomenda-se aprofundar o tuning dos modelos, explorar mais features e aplicar as soluções em ambiente real para maximizar o valor de negócio.\n",
    "\n",
    "**Obrigado!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Evaluation and Performance Analysis\n",
    "from sklearn.metrics import explained_variance_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurações visuais para melhorar a aparência dos gráficos\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('viridis')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Garantir que pipe_rf está definido e treinado\n",
    "try:\n",
    "    pipe_rf\n",
    "except NameError:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    pipe_rf = Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('reg', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    pipe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Obter nomes das features após o pré-processamento\n",
    "num_features = preprocessor.named_transformers_['num'].get_feature_names_out(num_cols)\n",
    "cat_features = preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_cols)\n",
    "feature_names = list(num_features) + list(cat_features)\n",
    "\n",
    "# Garantir que o tamanho bate com o feature_importances_\n",
    "print('Nomes de features:', len(feature_names), '| Importâncias:', len(pipe_rf.named_steps['reg'].feature_importances_))\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': pipe_rf.named_steps['reg'].feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# 1. Gráfico melhorado de importância das features - Top 15 features apenas para maior clareza\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "ax = sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "\n",
    "# Adicionar valores nas barras para melhor legibilidade\n",
    "for i, v in enumerate(top_features['importance']):\n",
    "    ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.title('Top 15 Features - Importância no Modelo Random Forest', fontsize=16)\n",
    "plt.xlabel('Importância', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate explained variance (R²) using cross-validation\n",
    "cv_scores = cross_val_score(pipe_rf, X_train, y_train, cv=5, scoring='r2')\n",
    "print(\"\\nCross-validation R² scores:\", cv_scores)\n",
    "print(\"Mean R²:\", cv_scores.mean())\n",
    "print(\"Standard deviation of R²:\", cv_scores.std())\n",
    "\n",
    "# Calculate residuals\n",
    "y_pred = pipe_rf.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# 2. Gráfico melhorado de resíduos vs valores previstos\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Criar um scatter plot com um colormap representando a densidade de pontos\n",
    "# Isso ajuda a visualizar melhor os padrões quando há muitos pontos\n",
    "sc = plt.scatter(y_pred, residuals, alpha=0.5, c=residuals, cmap='coolwarm', s=50, edgecolor='k', linewidth=0.5)\n",
    "plt.colorbar(sc, label='Valor do Resíduo')\n",
    "\n",
    "# Adicionar linha horizontal em y=0\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Resíduo Zero')\n",
    "\n",
    "# Adicionar limites de +/- 2 desvios padrão\n",
    "std_resid = np.std(residuals)\n",
    "plt.axhline(y=2*std_resid, color='gray', linestyle=':', linewidth=1, label='+2 Desvios Padrão')\n",
    "plt.axhline(y=-2*std_resid, color='gray', linestyle=':', linewidth=1, label='-2 Desvios Padrão')\n",
    "\n",
    "# Adicionar texto explicativo para os outliers\n",
    "outliers = sum((residuals > 2*std_resid) | (residuals < -2*std_resid))\n",
    "pct_outliers = (outliers / len(residuals)) * 100\n",
    "plt.annotate(f'Outliers: {outliers} ({pct_outliers:.1f}%)', \n",
    "             xy=(0.02, 0.95), xycoords='axes fraction',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.title('Resíduos vs Valores Previstos', fontsize=16)\n",
    "plt.xlabel('Tempo de Entrega Previsto (dias)', fontsize=14)\n",
    "plt.ylabel('Resíduos (dias)', fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Histograma melhorado da distribuição dos resíduos\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Usar distplot com mais bins para melhor visualização da distribuição\n",
    "# O uso de KDE ajuda a visualizar a forma da distribuição\n",
    "sns.histplot(residuals, bins=50, kde=True, color='skyblue', stat='density')\n",
    "\n",
    "# Adicionar uma curva normal teórica para comparação\n",
    "import scipy.stats as stats\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, np.mean(residuals), np.std(residuals))\n",
    "plt.plot(x, p, 'k--', linewidth=2, label='Distribuição Normal Teórica')\n",
    "\n",
    "# Adicionar linhas verticais para a média e medianas\n",
    "plt.axvline(np.mean(residuals), color='red', linestyle='-', linewidth=2, label=f'Média: {np.mean(residuals):.2f}')\n",
    "plt.axvline(np.median(residuals), color='green', linestyle='-', linewidth=2, label=f'Mediana: {np.median(residuals):.2f}')\n",
    "\n",
    "# Destacar os quantis para análise da distribuição\n",
    "for q, color, label in zip([0.25, 0.75], ['purple', 'orange'], ['Q1', 'Q3']):\n",
    "    quantile_val = np.quantile(residuals, q)\n",
    "    plt.axvline(quantile_val, color=color, linestyle=':', linewidth=1.5, \n",
    "                label=f'{label}: {quantile_val:.2f}')\n",
    "\n",
    "plt.title('Distribuição dos Resíduos', fontsize=16)\n",
    "plt.xlabel('Valor dos Resíduos', fontsize=14)\n",
    "plt.ylabel('Densidade', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Adicionar um novo gráfico: Q-Q plot para verificar normalidade dos resíduos\n",
    "plt.figure(figsize=(10, 10))\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot dos Resíduos', fontsize=16)\n",
    "plt.xlabel('Quantis Teóricos', fontsize=14)\n",
    "plt.ylabel('Quantis Amostrais', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics of residuals com formatação melhorada\n",
    "print(\"\\nEstatísticas dos Resíduos:\")\n",
    "res_stats = pd.Series(residuals).describe()\n",
    "for stat, value in zip(res_stats.index, res_stats.values):\n",
    "    print(f\"{stat.capitalize():>15}: {value:.4f}\")\n",
    "\n",
    "# 5. Box plot dos resíduos agrupados por mesma região (same_state)\n",
    "# Este gráfico adicional mostra como os resíduos se comportam por grupo\n",
    "test_df = pd.DataFrame({'residuals': residuals, 'same_state': X_test['same_state']})\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=test_df, x='same_state', y='residuals', palette='Set2')\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=1.5)\n",
    "plt.title('Distribuição dos Resíduos por Estado', fontsize=16)\n",
    "plt.xlabel('Mesmo Estado (1) vs Estados Diferentes (0)', fontsize=14)\n",
    "plt.ylabel('Resíduos', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae34074",
   "metadata": {},
   "source": [
    "# Análise de Desempenho do Modelo\n",
    "\n",
    "## Principais Descobertas:\n",
    "\n",
    "1. **Importância das Features**\n",
    "   - As características mais importantes para prever o tempo de entrega são:\n",
    "   - Fatores geográficos (mesmo estado, mesma cidade)\n",
    "   - Características do produto (densidade, volume)\n",
    "   - Métricas históricas de desempenho (tempo médio de entrega do vendedor)\n",
    "\n",
    "2. **Desempenho do Modelo**\n",
    "   - O modelo Random Forest demonstrou bom poder preditivo com:\n",
    "   - Pontuações R² consistentes em todas as dobras de validação cruzada\n",
    "   - Distribuição de resíduos relativamente simétrica\n",
    "   - Ausência de padrões fortes nos resíduos versus valores previstos\n",
    "\n",
    "3. **Insights de Negócio**\n",
    "   - O tempo de entrega é fortemente influenciado por:\n",
    "     - Proximidade geográfica entre vendedor e cliente\n",
    "     - Características físicas do produto\n",
    "     - Histórico de desempenho do vendedor\n",
    "   - Esses insights podem ser utilizados para:\n",
    "     - Otimizar o pareamento vendedor-cliente\n",
    "     - Estabelecer estimativas de entrega mais precisas\n",
    "     - Identificar áreas para melhoria logística\n",
    "\n",
    "4. **Recomendações**\n",
    "   - Focar na expansão da rede de vendedores em estados mal atendidos\n",
    "   - Considerar características do produto ao definir estimativas de entrega\n",
    "   - Utilizar o histórico de desempenho do vendedor para melhorar expectativas do cliente\n",
    "   - Implementar melhorias direcionadas para regiões com baixo desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Cluster Analysis\n",
    "\n",
    "# Calculate cluster characteristics\n",
    "cluster_stats = pd.DataFrame()\n",
    "cluster_stats['size'] = pd.Series(labels_kmeans).value_counts()\n",
    "cluster_stats['percentage'] = cluster_stats['size'] / len(labels_kmeans) * 100\n",
    "\n",
    "# Calculate mean values for each feature by cluster\n",
    "cluster_features = pd.DataFrame(X_unsup_proc, columns=anomaly_features)\n",
    "cluster_features['cluster'] = labels_kmeans\n",
    "cluster_means = cluster_features.groupby('cluster').mean()\n",
    "\n",
    "# Criar um gráfico 3D interativo que mostra relação entre tempo de entrega, valor do frete e peso do produto\n",
    "# Primeiro, vamos preparar um dataset com essas informações\n",
    "delivery_data = df_sup[['delivery_time', 'freight_value', 'product_weight_g', 'same_state']].copy()\n",
    "delivery_data['same_state_label'] = delivery_data['same_state'].map({1: 'Mesmo Estado', 0: 'Estados Diferentes'})\n",
    "\n",
    "# Criar o gráfico 3D interativo com plotly\n",
    "fig = px.scatter_3d(\n",
    "    delivery_data.sample(1000), # Amostra para melhorar a performance\n",
    "    x='delivery_time',\n",
    "    y='freight_value',\n",
    "    z='product_weight_g',\n",
    "    color='same_state_label',\n",
    "    opacity=0.7,\n",
    "    title='Relação entre Tempo de Entrega, Valor do Frete e Peso do Produto',\n",
    "    labels={\n",
    "        'delivery_time': 'Tempo de Entrega (dias)',\n",
    "        'freight_value': 'Valor do Frete (R$)',\n",
    "        'product_weight_g': 'Peso do Produto (g)',\n",
    "        'same_state_label': 'Localização'\n",
    "    },\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1\n",
    ")\n",
    "\n",
    "# Personalizar o layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Tempo de Entrega (dias)',\n",
    "        yaxis_title='Valor do Frete (R$)',\n",
    "        zaxis_title='Peso do Produto (g)',\n",
    "    ),\n",
    "    legend_title_text='Localização',\n",
    "    margin=dict(l=0, r=0, b=0, t=40),\n",
    ")\n",
    "\n",
    "# Adicionar uma linha de tendência para melhor visualização\n",
    "x_lines = delivery_data['delivery_time'].sample(20)\n",
    "y_lines = delivery_data['freight_value'].sample(20)\n",
    "z_lines = delivery_data['product_weight_g'].sample(20)\n",
    "\n",
    "# Ajustar um plano 3D aos dados para visualizar tendências\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "X_plane = delivery_data[['delivery_time', 'freight_value']].sample(500)\n",
    "y_plane = delivery_data['product_weight_g'].iloc[X_plane.index]\n",
    "model.fit(X_plane, y_plane)\n",
    "\n",
    "# Criar uma grade para o plano\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(delivery_data['delivery_time'].min(), delivery_data['delivery_time'].max(), 10),\n",
    "    np.linspace(delivery_data['freight_value'].min(), delivery_data['freight_value'].max(), 10)\n",
    ")\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "# Adicionar o plano ao gráfico\n",
    "fig.add_trace(go.Surface(\n",
    "    x=xx, y=yy, z=zz,\n",
    "    colorscale='Viridis',\n",
    "    opacity=0.5,\n",
    "    showscale=False\n",
    "))\n",
    "\n",
    "# Exibir o gráfico interativo\n",
    "fig.show()\n",
    "\n",
    "# Visualization of cluster characteristics\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot 2: Feature Means by Cluster\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.heatmap(cluster_means, cmap='YlOrRd', annot=True, fmt='.2f')\n",
    "plt.title('Mean Feature Values by Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster insights\n",
    "print(\"\\nCluster Characteristics:\")\n",
    "for cluster in range(len(cluster_stats)):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    print(f\"Size: {cluster_stats.loc[cluster, 'size']} ({cluster_stats.loc[cluster, 'percentage']:.1f}%)\")\n",
    "    print(\"Key characteristics:\")\n",
    "    # Get top 3 distinctive features for this cluster\n",
    "    distinctive_features = cluster_means.loc[cluster].sort_values(ascending=False)[:3]\n",
    "    for feat, val in distinctive_features.items():\n",
    "        print(f\"- High {feat}: {val:.2f}\")\n",
    "\n",
    "# Evaluate cluster quality\n",
    "if hasattr(kmeans, 'inertia_'):\n",
    "    print(f\"\\nCluster Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X_unsup_proc, labels_kmeans):.2f}\")\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin_score(X_unsup_proc_dense, labels_kmeans):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53933ca0",
   "metadata": {},
   "source": [
    "# Conclusões Finais e Recomendações\n",
    "\n",
    "## Aprendizado Supervisionado: Previsão do Tempo de Entrega\n",
    "\n",
    "1. **Desempenho do Modelo**\n",
    "   - O modelo Random Forest alcançou bom desempenho preditivo\n",
    "   - Características geográficas e do produto são os preditores mais fortes\n",
    "   - O modelo pode estimar tempos de entrega de forma confiável dentro de margens razoáveis\n",
    "\n",
    "2. **Impacto para o Negócio**\n",
    "   - Estimativas de entrega mais precisas podem melhorar a satisfação do cliente\n",
    "   - Insights sobre os principais fatores de tempo de entrega permitem melhorias direcionadas\n",
    "   - Potencial para previsões dinâmicas de tempo de entrega baseadas em dados em tempo real\n",
    "\n",
    "## Aprendizado Não Supervisionado: Segmentação de Vendedores\n",
    "\n",
    "1. **Análise de Clusters**\n",
    "   - Identificados segmentos distintos de vendedores com características únicas\n",
    "   - Encontrados padrões potencialmente problemáticos em alguns clusters de vendedores\n",
    "   - Concentração geográfica e métricas de desempenho revelam oportunidades de otimização\n",
    "\n",
    "2. **Recomendações para o Negócio**\n",
    "   - Implementar intervenções direcionadas para clusters de vendedores de alto risco\n",
    "   - Desenvolver programas de melhoria de desempenho para vendedores com base nas características do cluster\n",
    "   - Usar insights de clustering para integração e monitoramento de vendedores\n",
    "\n",
    "## Próximos Passos\n",
    "\n",
    "1. **Melhorias no Modelo**\n",
    "   - Incorporar mais características (clima, sazonalidade, etc.)\n",
    "   - Implementar atualizações de previsão em tempo real\n",
    "   - Retreinamento regular do modelo com novos dados\n",
    "\n",
    "2. **Implementação no Negócio**\n",
    "   - Integrar previsões nos sistemas voltados para o cliente\n",
    "   - Desenvolver monitoramento automatizado de vendedores com base na análise de clusters\n",
    "   - Criar dashboard para acompanhamento de métricas-chave e anomalias\n",
    "\n",
    "3. **Monitoramento e Manutenção**\n",
    "   - Configurar monitoramento regular do desempenho do modelo\n",
    "   - Acompanhar KPIs de negócio impactados pelos modelos\n",
    "   - Coletar feedback continuamente para futuras melhorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e63269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações aprimoradas para análise de cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "\n",
    "# 1. Visualização da distribuição de tamanhos dos clusters com gráfico interativo\n",
    "cluster_counts = pd.Series(labels_kmeans).value_counts().sort_index()\n",
    "cluster_percentages = (cluster_counts / len(labels_kmeans) * 100).round(1)\n",
    "labels = [f'Cluster {i}\\n{p}%' for i, p in enumerate(cluster_percentages)]\n",
    "\n",
    "# Visualização com Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(cluster_counts)))\n",
    "plt.pie(cluster_counts, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors,\n",
    "        wedgeprops={'width': 0.5, 'edgecolor': 'w', 'linewidth': 2})\n",
    "plt.title('Cluster Size Distribution', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Visualização aprimorada de features por cluster com heatmap interativo\n",
    "cluster_features = pd.DataFrame(X_unsup_proc, columns=anomaly_features)\n",
    "cluster_features['cluster'] = labels_kmeans\n",
    "cluster_means = cluster_features.groupby('cluster').mean()\n",
    "\n",
    "# Normalizar o heatmap para melhor visualização\n",
    "cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.heatmap(cluster_means_normalized, cmap='RdBu_r', annot=True, fmt='.2f', \n",
    "            linewidths=.5, center=0, cbar_kws={'label': 'Valor Normalizado (Z-score)'})\n",
    "plt.title('Mean Feature Values by Cluster', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Clusters', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Análise silhouette para avaliação da qualidade dos clusters\n",
    "silhouette_vals = silhouette_samples(X_unsup_proc, labels_kmeans)\n",
    "y_ticks = []\n",
    "y_lower, y_upper = 0, 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = plt.cm.get_cmap('tab10')\n",
    "\n",
    "for i, cluster in enumerate(np.unique(labels_kmeans)):\n",
    "    cluster_silhouette_vals = silhouette_vals[labels_kmeans == cluster]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    y_upper += len(cluster_silhouette_vals)\n",
    "    color = cm(float(i) / len(np.unique(labels_kmeans)))\n",
    "    ax.barh(range(y_lower, y_upper), cluster_silhouette_vals, height=1.0, \n",
    "            edgecolor='none', color=color, alpha=0.8)\n",
    "    y_ticks.append((y_lower + y_upper) / 2)\n",
    "    y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "silhouette_avg = silhouette_score(X_unsup_proc, labels_kmeans)\n",
    "ax.axvline(silhouette_avg, color='red', linestyle='--', \n",
    "           label=f'Média: {silhouette_avg:.2f}')\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels([f'Cluster {i}' for i in range(len(np.unique(labels_kmeans)))])\n",
    "ax.set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Cluster', fontsize=12)\n",
    "ax.set_title('Análise Silhouette por Cluster', fontsize=16)\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Visualização 3D dos clusters com PCA para 3 componentes\n",
    "pca_3d = PCA(n_components=3, random_state=42)\n",
    "X_pca_3d = pca_3d.fit_transform(X_unsup_proc)\n",
    "\n",
    "# Criando um DataFrame para facilitar a plotagem\n",
    "df_pca_3d = pd.DataFrame(X_pca_3d, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_3d['Cluster'] = labels_kmeans\n",
    "\n",
    "# Exibir a variância explicada por cada componente\n",
    "explained_variance = pca_3d.explained_variance_ratio_ * 100\n",
    "print(f\"Variância explicada pelas componentes principais:\")\n",
    "for i, variance in enumerate(explained_variance):\n",
    "    print(f\"PC{i+1}: {variance:.2f}%\")\n",
    "print(f\"Variância total explicada: {sum(explained_variance):.2f}%\")\n",
    "\n",
    "# Criando uma visualização 3D interativa usando plotly.express\n",
    "fig = px.scatter_3d(\n",
    "    df_pca_3d,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    z='PC3',\n",
    "    color='Cluster',  # Colorir os pontos com base no cluster\n",
    "    opacity=0.7,\n",
    "    title='Visualização 3D dos Clusters com PCA (Interativo)',\n",
    "    labels={\n",
    "        'PC1': f'PC1 ({explained_variance[0]:.2f}%)',\n",
    "        'PC2': f'PC2 ({explained_variance[1]:.2f}%)',\n",
    "        'PC3': f'PC3 ({explained_variance[2]:.2f}%)'\n",
    "    },\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1\n",
    ")\n",
    "\n",
    "# Atualizar layout para um melhor visual\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=40),\n",
    "    legend_title_text='Clusters',\n",
    "    scene=dict(\n",
    "        xaxis_title=f'PC1 ({explained_variance[0]:.2f}%)',\n",
    "        yaxis_title=f'PC2 ({explained_variance[1]:.2f}%)',\n",
    "        zaxis_title=f'PC3 ({explained_variance[2]:.2f}%)'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Exibir o gráfico interativo\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "#     \n",
    "# colors = plt.cm.tab10(np.linspace(0, 1, len(np.unique(labels_kmeans))))\n",
    "# cmap = ListedColormap(colors)\n",
    "#     \n",
    "# scatter = ax.scatter(df_pca_3d['PC1'], df_pca_3d['PC2'], df_pca_3d['PC3'], \n",
    "#                      c=df_pca_3d['Cluster'], cmap=cmap, s=30, alpha=0.7)\n",
    "#     \n",
    "# # Adicionar uma legenda\n",
    "# legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "# ax.add_artist(legend1)\n",
    "#     \n",
    "# # Rótulos e título\n",
    "# ax.set_xlabel(f'PC1 ({explained_variance[0]:.2f}%)', fontsize=12)\n",
    "# ax.set_ylabel(f'PC2 ({explained_variance[1]:.2f}%)', fontsize=12)\n",
    "# ax.set_zlabel(f'PC3 ({explained_variance[2]:.2f}%)', fontsize=12)\n",
    "# ax.set_title('Visualização 3D dos Clusters com PCA', fontsize=16)\n",
    "#     \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# 5. Análise das características distintivas de cada cluster\n",
    "# Determinar as características mais distintivas por cluster\n",
    "cluster_importance = pd.DataFrame()\n",
    "\n",
    "for cluster in sorted(np.unique(labels_kmeans)):\n",
    "    # Calcular a diferença entre a média do cluster e a média global para cada feature\n",
    "    cluster_diff = cluster_means.loc[cluster] - cluster_features[anomaly_features].mean()\n",
    "    # Normalizar pela desviação padrão global\n",
    "    cluster_diff_norm = cluster_diff / cluster_features[anomaly_features].std()\n",
    "    # Adicionar ao DataFrame\n",
    "    cluster_importance[f'Cluster {cluster}'] = cluster_diff_norm\n",
    "\n",
    "# Visualizar as características mais importantes por cluster\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(cluster_importance, annot=True, cmap='RdBu_r', fmt='.2f', center=0,\n",
    "           linewidths=.5, cbar_kws={'label': 'Diferença Normalizada (Z-score)'})\n",
    "plt.title('Características Distintivas por Cluster', fontsize=16)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Sumário interpretativo dos clusters\n",
    "print(\"\\n===== INTERPRETAÇÃO DOS CLUSTERS =====\\n\")\n",
    "\n",
    "for cluster in sorted(np.unique(labels_kmeans)):\n",
    "    # Selecionar as 3 características mais positivas e negativas para este cluster\n",
    "    cluster_features_sorted = cluster_importance[f'Cluster {cluster}'].sort_values()\n",
    "    top_negative = cluster_features_sorted.head(3)\n",
    "    top_positive = cluster_features_sorted.tail(3)\n",
    "    \n",
    "    # Calcular o tamanho e porcentagem do cluster\n",
    "    cluster_size = sum(labels_kmeans == cluster)\n",
    "    cluster_percent = (cluster_size / len(labels_kmeans) * 100)\n",
    "    \n",
    "    print(f\"Cluster {cluster} ({cluster_size} elementos, {cluster_percent:.1f}% do total):\")\n",
    "    print(f\"  Características distintivas positivas:\")\n",
    "    for feat, val in top_positive.items():\n",
    "        print(f\"    - {feat}: {val:.2f} desvios padrão acima da média\")\n",
    "    print(f\"  Características distintivas negativas:\")\n",
    "    for feat, val in top_negative.items():\n",
    "        print(f\"    - {feat}: {val:.2f} desvios padrão abaixo da média\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1dd86",
   "metadata": {},
   "source": [
    "# Análise Detalhada dos Clusters de Vendedores\n",
    "\n",
    "## Características dos Clusters Identificados\n",
    "\n",
    "Após a aplicação do algoritmo K-Means, identificamos quatro clusters distintos de vendedores com as seguintes características:\n",
    "\n",
    "### Cluster 0 (11,1% dos vendedores):\n",
    "- **Pontos fortes**: Concentração geográfica elevada (0,38), indicando operação regionalizada\n",
    "- **Pontos fracos**: Avaliações muito baixas (Z-score -2,11), sugerindo problemas de qualidade\n",
    "- **Interpretação**: \"Vendedores regionais com problemas de satisfação do cliente\" - Este grupo opera em regiões específicas com baixo desempenho em avaliações\n",
    "\n",
    "### Cluster 1 (77,0% dos vendedores):\n",
    "- **Pontos fortes**: Avaliações positivas (Z-score 0,32), boa distribuição geográfica\n",
    "- **Pontos fracos**: Crescimento moderado e preços medianos\n",
    "- **Interpretação**: \"Vendedores regulares\" - Representam a maioria do marketplace, com desempenho estável e avaliações satisfatórias\n",
    "\n",
    "### Cluster 2 (7,9% dos vendedores):\n",
    "- **Pontos fortes**: Crescimento extraordinário (Z-score 2,49), alto volume de vendas (1,53)\n",
    "- **Pontos fracos**: Concentração geográfica negativa (-0,59), sugerindo distribuição muito ampla\n",
    "- **Interpretação**: \"Vendedores em expansão rápida\" - Grupo em crescimento acelerado, possivelmente com estratégias agressivas de expansão\n",
    "\n",
    "### Cluster 3 (4,0% dos vendedores):\n",
    "- **Pontos fortes**: Preços muito elevados (Z-score 3,36), alto desvio padrão nos preços (3,14)\n",
    "- **Pontos fracos**: Crescimento limitado (-0,27)\n",
    "- **Interpretação**: \"Vendedores premium\" - Focados em produtos de alto valor, possivelmente em nichos específicos\n",
    "\n",
    "## Métricas de Qualidade dos Clusters\n",
    "\n",
    "- **Silhouette Score**: 0,38 - Indica separação moderadamente boa entre clusters\n",
    "- **Davies-Bouldin Score**: 1,14 - Valor relativamente baixo, sugerindo clusters bem definidos\n",
    "- **Inércia**: 5394,03 - Medida de compactação interna dos clusters\n",
    "\n",
    "## Visualização dos Clusters\n",
    "\n",
    "A visualização 3D dos clusters através de PCA mostra uma boa separação espacial, com:\n",
    "- **PC1**: 26,94% da variância explicada - Relacionada principalmente a preço e volume\n",
    "- **PC2**: 21,70% da variância explicada - Relacionada a avaliações e crescimento\n",
    "- **PC3**: 20,02% da variância explicada - Relacionada a concentração geográfica\n",
    "- **Variância total explicada**: 68,66% - Um bom percentual para visualização e interpretação\n",
    "\n",
    "## Recomendações Estratégicas por Cluster\n",
    "\n",
    "### Para Cluster 0 (Vendedores problemáticos):\n",
    "- Implementar programa de melhoria de qualidade com monitoramento rigoroso\n",
    "- Oferecer treinamentos específicos sobre atendimento ao cliente\n",
    "- Avaliar possível descontinuação se não houver melhora\n",
    "\n",
    "### Para Cluster 1 (Vendedores regulares):\n",
    "- Incentivar crescimento com programas de fidelidade\n",
    "- Fornecer ferramentas de automação para escalar operações\n",
    "- Reconhecer desempenho consistente com benefícios no marketplace\n",
    "\n",
    "### Para Cluster 2 (Vendedores em expansão):\n",
    "- Monitorar qualidade durante o crescimento acelerado\n",
    "- Oferecer suporte logístico para manter a escalabilidade\n",
    "- Verificar sustentabilidade do modelo de crescimento\n",
    "\n",
    "### Para Cluster 3 (Vendedores premium):\n",
    "- Desenvolver seção exclusiva no marketplace para produtos premium\n",
    "- Oferecer serviços especiais para este segmento (embalagem premium, entrega prioritária)\n",
    "- Analisar estratégias para ampliar base de clientes mantendo posicionamento premium"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
